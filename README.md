## Project 6:  Distributed Key-Value Database
### Description  
In this project, we will build a (relatively) simple, distributed, replicated key-value datastore. A key-value datastore is a very simple type of database that supports two API calls from clients: put(key, value) and get(key). The former API allows a client application to store a key-value pair in the database, while the latter API allows a client to retrieve a previously stored value by supplying its key. Real-world examples of distributed key-value data stores include Memcached, Redis, DynamoDB, etc.

The system will be replicated and support strong consistency guarantees. Thus, we will be implementing a simplified version of the Raft consensus protocol. The data store will be run multiple times, in parallel, and it will use the Raft protocol to maintain consensus among the replicas.
### Approach  
We began with the code provided by the professors, which implements the connection for all the replicas and tells the rest of them they just started. It also creates the method _send_, which sends messages through the corresponding sockets, and the method _run_ that we will use to read the received messages and perform the corresponding functionalities.

We proceeded by following the order advised by the professors on the course website, so first, we began by reading the messages from the client, that is, reading the corresponding _get_ and _put_ messages for which we created two separate methods that do the corresponding tasks.
- _get_received_ will redirect the message in case a replica that is not a leader received it and otherwise, it will respond with the current state machine from the leader.
- In the same way, _put_received_ will also redirect the message in case it was not intended for that replica, else, will store the received message in the leader’s log and send an _AppendEntry_ to the rest of the replicas with the _send_AppendEntry_ method.
Prior to doing all those things, a new leader has to be elected. To do so, we set a timer for each replica, and if it runs out, a new election will begin. The replica that timed out will set itself to candidate state,  will increase the timer, and will create a variable to store the number of votes it received, lastly, it will send a message requesting the vote to the rest of the replicas and wait for the majority to vote for them, unless another replica times out and the whole process starts again. To be elected as a leader, the replica’s log has to be up to date, which is checked by the _candidateUpToDate_ method.

Going back to the _send_AppendEntry_ method, it will broadcast the log and the previous log entry to the last one so each replica can update their log and store that entry. They will do so with the _update_log_ method. This method will try to match the previous entry to the one we are trying to store in the replica’s log and will keep trying until they find a match, and therefore will proceed to store all those entries that they did not have. Once the match is found, a message will be sent to the leader so it knows the entry was added to the replica’s log. If the leader receives this message from the majority, i.e. ```(len(self.others) + 1) // 2 + 1```, it will proceed to update its state machine, reply to the client saying it has committed that entry, and of course, committing that entry in the replicas which they will only accept if the leader is from a higher or the same term that they have.
### Challenges and difficulties
The most difficult part of this project was debugging and getting an appropriate performance. We also struggled to find information about the Raft protocol as what we saw in class was very poor for what we needed to develop in the project. Nevertheless, the Raft paper was remarkably helpful, even though it was not always enough and further research was required. Also add, that we really struggled with the time as it required a lot of testing and trial and error and with the rest of the classes, it was hard to manage.
Overall, the project, as hard as it was, helped us get a better understanding of the whole protocol and it was also valuable to get a full understanding of the protocol.
### Testing and debugging
This was the hardest part as it all had to be done by printing the results in the terminal. What made it challenging, is that the messages received did not print in order so trying to make sense out of it was tough.
Another thing that was demanding, was understanding the results from the tests provided by the professors, as the information they returned sometimes was confusing, and there were not any examples explaining what each thing meant.
